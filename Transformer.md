## Transformer  
使用attention组成了encoder-decoder的框架，并将其用于机器翻译——Attention in All Tou Need(NIPS2017)
### Self-Attention Layer  
使用Self-Attention取代RNN